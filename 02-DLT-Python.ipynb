{"cells":[{"cell_type":"markdown","source":["-sandbox\n# Simplify ETL with Delta Live Table\n\nDLT makes Data Engineering accessible for all. Just declare your transformations in SQL or Python, and DLT will handle the Data Engineering complexity for you.\n\n<img style=\"float:right\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-1.png\" width=\"700\"/>\n\n**Accelerate ETL development** <br/>\nEnable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance \n\n**Remove operational complexity** <br/>\nBy automating complex administrative tasks and gaining broader visibility into pipeline operations\n\n**Trust your data** <br/>\nWith built-in quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML \n\n**Simplify batch and streaming** <br/>\nWith self-optimization and auto-scaling data pipelines for batch or streaming processing \n\n## Our Delta Live Table pipeline\n\nWe'll be using as input a raw dataset containing information on our customers Loan and historical transactions. \n\nOur goal is to ingest this data in near real time and build table for our Analyst team while ensuring data quality.\n\n<!-- do not remove -->\n<img width=\"1px\" src=\"https://www.google-analytics.com/collect?v=1&gtm=GTM-NKQ8TT7&tid=UA-163989034-1&cid=555&aip=1&t=event&ec=field_demos&ea=display&dp=%2F42_field_demos%2Ffeatures%2Fdlt%2Fnotebook_dlt_sql&dt=DLT\">\n<!-- [metadata={\"description\":\"Full DLT demo, going into details. Use loan dataset\",\n \"authors\":[\"dillon.bostwick@databricks.com\"],\n \"db_resources\":{},\n  \"search_tags\":{\"vertical\": \"retail\", \"step\": \"Data Engineering\", \"components\": [\"autoloader\", \"dlt\"]}}] -->"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6b972f0-3c53-45ba-8288-fecf11413cf1"}}},{"cell_type":"markdown","source":["-sandbox \n\n## Bronze layer: incrementally ingest data leveraging Databricks Autoloader\n\n<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-2.png\" width=\"500\"/>\n\nOur raw data is being sent to a blob storage. \n\nAutoloader simplify this ingestion, including schema inference, schema evolution while being able to scale to millions of incoming files. \n\nAutoloader is available in Python using the `cloud_files` format and can be used with a variety of format (json, csv, avro...):\n\n\n#### STREAMING LIVE TABLE \nDefining tables as `STREAMING` will guarantee that you only consume new incoming data. Without `STREAMING`, you will scan and ingest all the data available at once. See the [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-incremental-data.html) for more details"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50eacc05-8fb7-4488-adf1-1a4cfc1d35df"}}},{"cell_type":"code","source":["import dlt\nfrom pyspark.sql import functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4278d35e-76e7-47e5-940a-6c8b3c02cb1f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from json.`/demo/dlt_loan/landing`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f832f5d9-fa05-40d1-bd98-1ebed02019af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"New raw loan data incrementally ingested from cloud object storage landing zone\")\ndef BZ_raw_txs():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n      .load(\"/demo/dlt_loan/landing\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e2ae834-bdfb-4239-ba86-b0f724083d31"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"Lookup mapping for accounting codes\")\ndef ref_accounting_treatment():\n  return spark.read.format(\"delta\").load(\"/demo/dlt_loan/ref_accounting_treatment/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bd12600-f3bd-440e-b355-dbd664ae5375"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"Raw historical transactions\")\ndef BZ_reference_loan_stats():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"csv\")\n      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n      .load(\"/databricks-datasets/lending-club-loan-stats/LoanStats_*\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41f78453-7639-4d29-8bd9-f0b0b6075f0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox \n\n## Silver layer: joining tables while ensuring data quality\n\n<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-3.png\" width=\"500\"/>\n\nOnce the bronze layer is defined, we'll create the sliver layers by Joining data. Note that bronze tables are referenced using the `LIVE` spacename. \n\nTo consume only increment from the Bronze layer like `BZ_raw_txs`, we'll be using the `read_stream` function: `dlt.read_stream(\"BZ_raw_txs\")`\n\nNote that we don't have to worry about compactions, DLT handles that for us.\n\n#### Expectations\nBy defining expectations (`@dlt.expect`), you can enforce and track your data quality. See the [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html) for more details"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54346798-0b91-446e-a99f-1f36755b12c0"}}},{"cell_type":"code","source":["@dlt.table(comment=\"Livestream of new transactions, cleaned and compliant\")\n@dlt.expect(\"Payments should be this year\", \"(next_payment_date > date('2020-12-31'))\")\n@dlt.expect_or_drop(\"Balance should be positive\", \"(balance > 0 AND arrears_balance > 0)\")\n@dlt.expect_or_fail(\"Cost center must be specified\", \"(cost_center_code IS NOT NULL)\")\ndef SV_cleaned_new_txs():\n  txs = dlt.read_stream(\"BZ_raw_txs\").alias(\"txs\")\n  rat = dlt.read(\"ref_accounting_treatment\").alias(\"rat\")\n  return (\n    txs.join(rat, F.col(\"txs.accounting_treatment_id\") == F.col(\"rat.id\"), \"inner\")\n      .selectExpr(\"txs.*\", \"rat.id as accounting_treatment\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ff4892-9211-4c1c-a3a0-dc1731b57b6b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"Historical loan transactions\")\ndef SV_historical_txs():\n  rls = dlt.read_stream(\"BZ_reference_loan_stats\")\n  rat = dlt.read(\"ref_accounting_treatment\")\n  return (\n    rls.join(rat, \"id\", \"inner\")\n      .select(rls.columns)\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cdce362-80a9-4914-92d8-fc01863f9c32"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox \n\n## Gold layer\n\n<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-4.png\" width=\"500\"/>\n\nOur last step is to materialize the Gold Layer.\n\nBecause these tables will be requested at scale using a SQL Endpoint, we'll add Zorder at the table level to ensure faster queries using `pipelines.autoOptimize.zOrderCols`, and DLT will handle the rest."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a71f190e-6af5-4394-a79a-5e5aabccc28a"}}},{"cell_type":"code","source":["@dlt.table(\n  comment=\"Combines historical and new loan data for unified rollup of loan balances\",\n  table_properties={\"pipelines.autoOptimize.zOrderCols\": \"location_code\"})\ndef GL_total_loan_balances_1():\n  return (\n    dlt.read(\"SV_historical_txs\")\n      .groupBy(\"addr_state\")\n      .agg(F.sum(\"revol_bal\").alias(\"bal\"))\n      .withColumnRenamed(\"addr_state\", \"location_code\")\n      .union(\n        dlt.read(\"SV_cleaned_new_txs\")\n          .groupBy(\"country_code\")\n          .agg(F.sum(\"balance\").alias(\"bal\"))\n          .withColumnRenamed(\"country_code\", \"location_code\")\n      )          \n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c055b0cc-34e8-442c-a920-e37d0a944a89"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"Combines historical and new loan data for unified rollup of loan balances\")\ndef GL_total_loan_balances_2():\n  return (\n    dlt.read(\"SV_historical_txs\")\n      .groupBy(\"addr_state\")\n      .agg(F.sum(\"revol_bal\").alias(\"bal\"))\n      .withColumnRenamed(\"addr_state\", \"location_code\")\n      .union(\n        dlt.read(\"SV_cleaned_new_txs\")\n          .groupBy(\"country_code\")\n          .agg(F.sum(\"balance\").alias(\"bal\"))\n          .withColumnRenamed(\"country_code\", \"location_code\")\n      )\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdf3d977-d1ac-408d-a0a0-6a1ff6b6dbe4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(\n  comment=\"Live view of new loan balances for consumption by different cost centers\",\n  table_properties={\"pipelines.autoOptimize.zOrderCols\": \"cost_center_code\"})\ndef GL_new_loan_balances_by_cost_center():\n  return (\n    dlt.read(\"SV_cleaned_new_txs\")\n      .groupBy(\"cost_center_code\")\n      .agg(F.sum(\"balance\").alias(\"balance\"))\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48bf1601-50e0-4811-a3cc-9b0656914782"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(\n  comment=\"Live view of new loan balances per country\",\n  table_properties={\"pipelines.autoOptimize.zOrderCols\": \"country_code\"})\ndef GL_new_loan_balances_by_country():\n  return (\n    dlt.read(\"SV_cleaned_new_txs\")\n      .groupBy(\"country_code\")\n      .agg(F.sum(\"count\").alias(\"count\"))\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6bf74f8-599d-4969-8c00-a4882db67464"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next steps\n\nYour DLT pipeline is ready to be started.\n\nOpen the DLT menu, create a pipeline and select this notebook to run it. To generate sample data, please run the [companion notebook]($./00-Loan-Data-Generator) (make sure the path where you read and write the data are the same!)\n\nDatas Analyst can start using DBSQL to analyze data and track our Loan metrics.  Data Scientist can also access the data to start building models to predict payment default or other more advanced use-cases."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6066aed4-c5e5-4ca2-9075-f9a5123f29a6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02-DLT-Python","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":983711257711984}},"nbformat":4,"nbformat_minor":0}
